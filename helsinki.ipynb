{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-08T03:37:31.832140Z",
     "iopub.status.busy": "2025-10-08T03:37:31.831621Z",
     "iopub.status.idle": "2025-10-08T03:38:47.551189Z",
     "shell.execute_reply": "2025-10-08T03:38:47.550212Z",
     "shell.execute_reply.started": "2025-10-08T03:37:31.832117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# KAGGLE-COMPATIBLE PACKAGE INSTALLATION\n",
    "# ===================================================================\n",
    "# All packages are combined into a single command for efficiency.\n",
    "# The \"-q\" flag ensures a \"quiet\" installation with minimal output.\n",
    "\n",
    "!pip install -q transformers datasets evaluate pandas openpyxl torch accelerate \\\n",
    "               sentencepiece numpy sacrebleu rouge-score scipy scikit-learn\n",
    "\n",
    "print(\"✓ All packages installed successfully!\")\n",
    "print(\"✓ Enhanced training packages ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T03:38:47.552847Z",
     "iopub.status.busy": "2025-10-08T03:38:47.552621Z",
     "iopub.status.idle": "2025-10-08T03:38:47.572859Z",
     "shell.execute_reply": "2025-10-08T03:38:47.572029Z",
     "shell.execute_reply.started": "2025-10-08T03:38:47.552826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training file found in Kaggle input!\n",
      "✓ Validation file found in Kaggle input!\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 2: Access Data Files from Kaggle Dataset\n",
    "# ===================================================================\n",
    "import os\n",
    "\n",
    "# Updated with your specific dataset name\n",
    "dataset_name = 'lmtdata'\n",
    "\n",
    "# Define file paths in the Kaggle input directory\n",
    "train_file_path = f'/kaggle/input/lmtdata/english-hindi-train.xlsx'\n",
    "val_file_path = f'/kaggle/input/lmtdata/english-hindi-valid.xlsx'\n",
    "\n",
    "# Verify files exist\n",
    "if os.path.exists(train_file_path):\n",
    "    print(\"✓ Training file found in Kaggle input!\")\n",
    "else:\n",
    "    print(f\"✗ Training file not found! Please check the path: {train_file_path}\")\n",
    "\n",
    "if os.path.exists(val_file_path):\n",
    "    print(\"✓ Validation file found in Kaggle input!\")\n",
    "else:\n",
    "    print(f\"✗ Validation file not found! Please check the path: {val_file_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T03:38:47.573976Z",
     "iopub.status.busy": "2025-10-08T03:38:47.573761Z",
     "iopub.status.idle": "2025-10-08T03:39:12.562428Z",
     "shell.execute_reply": "2025-10-08T03:39:12.561634Z",
     "shell.execute_reply.started": "2025-10-08T03:38:47.573957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CELL 3: Import All Libraries and Setup Functions\n",
    "# ===================================================================\n",
    "# This cell is perfectly compatible with Kaggle. No changes are needed.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T03:39:12.565067Z",
     "iopub.status.busy": "2025-10-08T03:39:12.564402Z",
     "iopub.status.idle": "2025-10-08T03:39:12.575143Z",
     "shell.execute_reply": "2025-10-08T03:39:12.573875Z",
     "shell.execute_reply.started": "2025-10-08T03:39:12.565046Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loading functions defined!\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 4: Define Data Loading Functions\n",
    "# ===================================================================\n",
    "# This code is perfectly compatible with Kaggle. No changes are needed.\n",
    "# These functions use standard pandas and will work with the Kaggle file paths.\n",
    "\n",
    "def load_excel_data(file_path, sheet_name=0):\n",
    "    \"\"\"Load Excel/XLSX file and return DataFrame\"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def explore_translation_data(df, name):\n",
    "    \"\"\"Explore the structure and statistics of translation data\"\"\"\n",
    "    print(f\"\\n=== {name} Dataset Exploration ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "    # Check for missing values\n",
    "    print(f\"Missing values:\\n{df.isnull().sum()}\")\n",
    "\n",
    "    # Show sample data\n",
    "    print(f\"\\nSample data:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Check text lengths if source/translation columns exist\n",
    "    if 'Source' in df.columns or 'source' in df.columns or 'English' in df.columns:\n",
    "        source_cols = ['Source', 'source', 'English', 'english']\n",
    "        source_col = None\n",
    "        for col in source_cols:\n",
    "            if col in df.columns:\n",
    "                source_col = col\n",
    "                break\n",
    "\n",
    "        if source_col:\n",
    "            source_lengths = df[source_col].str.split().str.len()\n",
    "            print(f\"\\nSource text length stats (words):\")\n",
    "            print(f\"  Mean: {source_lengths.mean():.1f}\")\n",
    "            print(f\"  Median: {source_lengths.median():.1f}\")\n",
    "            print(f\"  Min: {source_lengths.min()}\")\n",
    "            print(f\"  Max: {source_lengths.max()}\")\n",
    "\n",
    "    if 'Translation' in df.columns or 'translation' in df.columns or 'Hindi' in df.columns:\n",
    "        trans_cols = ['Translation', 'translation', 'Hindi', 'hindi']\n",
    "        trans_col = None\n",
    "        for col in trans_cols:\n",
    "            if col in df.columns:\n",
    "                trans_col = col\n",
    "                break\n",
    "\n",
    "        if trans_col:\n",
    "            trans_lengths = df[trans_col].str.split().str.len()\n",
    "            print(f\"\\nTranslation text length stats (words):\")\n",
    "            print(f\"  Mean: {trans_lengths.mean():.1f}\")\n",
    "            print(f\"  Median: {trans_lengths.median():.1f}\")\n",
    "            print(f\"  Min: {trans_lengths.min()}\")\n",
    "            print(f\"  Max: {trans_lengths.max()}\")\n",
    "\n",
    "print(\"✓ Data loading functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T03:39:12.576593Z",
     "iopub.status.busy": "2025-10-08T03:39:12.575824Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from Kaggle dataset...\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 5: Load Data from Kaggle Dataset\n",
    "# ===================================================================\n",
    "# The function calls are correct; only the print statements are updated for clarity.\n",
    "\n",
    "print(\"Loading training data from Kaggle dataset...\")\n",
    "train_df = load_excel_data(train_file_path)\n",
    "\n",
    "print(\"\\nLoading validation data from Kaggle dataset...\")\n",
    "val_df = load_excel_data(val_file_path)\n",
    "\n",
    "# This exploration code works perfectly as it operates on the DataFrames.\n",
    "if train_df is not None:\n",
    "    explore_translation_data(train_df, \"Training\")\n",
    "if val_df is not None:\n",
    "    explore_translation_data(val_df, \"Validation\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CELL 6: Define Data Preprocessing Functions\n",
    "# ===================================================================\n",
    "# This code is perfectly compatible with Kaggle. No changes are needed.\n",
    "\n",
    "def prepare_translation_data(train_df, val_df):\n",
    "    \"\"\"Prepare data for training\"\"\"\n",
    "\n",
    "    # Try different possible column names\n",
    "    source_cols = ['Source', 'source', 'English', 'english', 'EN', 'en']\n",
    "    target_cols = ['Translation', 'translation', 'Hindi', 'hindi', 'HI', 'hi']\n",
    "\n",
    "    source_col = None\n",
    "    target_col = None\n",
    "\n",
    "    for col in source_cols:\n",
    "        if col in train_df.columns:\n",
    "            source_col = col\n",
    "            break\n",
    "\n",
    "    for col in target_cols:\n",
    "        if col in train_df.columns:\n",
    "            target_col = col\n",
    "            break\n",
    "\n",
    "    if source_col is None or target_col is None:\n",
    "        print(\"Error: Could not find source/target columns\")\n",
    "        print(f\"Available columns: {list(train_df.columns)}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    print(f\"Using source column: {source_col}\")\n",
    "    print(f\"Using target column: {target_col}\")\n",
    "\n",
    "    # Extract training data\n",
    "    train_source = train_df[source_col].fillna('').astype(str).tolist()\n",
    "    train_target = train_df[target_col].fillna('').astype(str).tolist()\n",
    "\n",
    "    # Extract validation data (only source text for prediction)\n",
    "    val_source_col = source_col if source_col in val_df.columns else None\n",
    "    if val_source_col is None:\n",
    "        for col in source_cols:\n",
    "            if col in val_df.columns:\n",
    "                val_source_col = col\n",
    "                break\n",
    "\n",
    "    if val_source_col is None:\n",
    "        print(\"Error: Could not find source column in validation data\")\n",
    "        print(f\"Available validation columns: {list(val_df.columns)}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    val_source = val_df[val_source_col].fillna('').astype(str).tolist()\n",
    "    val_ids = val_df['ID'].tolist() if 'ID' in val_df.columns else list(range(len(val_df)))\n",
    "\n",
    "    print(f\"Training pairs: {len(train_source)}\")\n",
    "    print(f\"Validation samples: {len(val_source)}\")\n",
    "\n",
    "    return train_source, train_target, val_source, val_ids\n",
    "\n",
    "print(\"✓ Data preprocessing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# ADDITIONAL OPTIMIZATIONS FOR OTHER CELLS\n",
    "# ===================================================================\n",
    "\n",
    "# ============= OPTIMIZATION 1: REPLACE CELL 7 =============\n",
    "# Enhanced Model Setup with Better Architecture Selection\n",
    "\n",
    "def setup_translation_model(model_name=\"Helsinki-NLP/opus-mt-en-hi\"):\n",
    "    \"\"\"Setup translation model with optimization for Hindi translation\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    # Try multiple models in order of preference for EN->HI translation\n",
    "    models_to_try = [\n",
    "        \"Helsinki-NLP/opus-mt-en-hi\",      # Best for EN->HI\n",
    "        \"ai4bharat/IndicTrans2-En-Indic-1B\",  # Specialized for Indian languages\n",
    "        \"google/mt5-base\",                  # Larger than mt5-small\n",
    "        \"facebook/mbart-large-50-many-to-many-mmt\"  # Multilingual\n",
    "    ]\n",
    "    \n",
    "    model_loaded = False\n",
    "    for try_model in models_to_try:\n",
    "        try:\n",
    "            print(f\"Attempting to load: {try_model}\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(try_model)\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(try_model)\n",
    "            model_name = try_model\n",
    "            model_loaded = True\n",
    "            print(f\"✓ Successfully loaded {try_model}\")\n",
    "            break\n",
    "        except:\n",
    "            print(f\"✗ Failed to load {try_model}, trying next...\")\n",
    "            continue\n",
    "    \n",
    "    if not model_loaded:\n",
    "        raise Exception(\"Could not load any translation model\")\n",
    "    \n",
    "    # Add special tokens if needed\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, tokenizer, model_name\n",
    "\n",
    "def create_translation_dataset(source_texts, target_texts, tokenizer, max_length=512):  # Increased from 256\n",
    "    \"\"\"Enhanced dataset creation with better preprocessing\"\"\"\n",
    "    \n",
    "    # Data cleaning function\n",
    "    def clean_text(text):\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        if isinstance(text, str):\n",
    "            # Remove HTML entities\n",
    "            text = text.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')\n",
    "            # Normalize whitespace\n",
    "            text = ' '.join(text.split())\n",
    "            # Add language tags if model supports it\n",
    "            if \"mbart\" in tokenizer.name_or_path.lower():\n",
    "                return f\"[en_XX] {text}\"  # Source language tag\n",
    "            return text\n",
    "        return \"\"\n",
    "    \n",
    "    # Clean texts\n",
    "    source_texts = [clean_text(text) for text in source_texts]\n",
    "    target_texts = [clean_text(text) for text in target_texts]\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        # Tokenize with optimal settings\n",
    "        model_inputs = tokenizer(\n",
    "            examples['source'],\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize targets with proper formatting\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                examples['target'],\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                padding=False,\n",
    "                add_special_tokens=True\n",
    "            )\n",
    "        \n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = HFDataset.from_dict({\n",
    "        'source': source_texts,\n",
    "        'target': target_texts\n",
    "    })\n",
    "    \n",
    "    # Tokenize with optimized batching\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function, \n",
    "        batched=True,\n",
    "        batch_size=1000,  # Process in larger batches\n",
    "        num_proc=2  # Use multiple processes if available\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CELL 9: Define Inference Functions\n",
    "# ===================================================================\n",
    "def translate_texts(model, tokenizer, source_texts, ids, max_length=256, batch_size=8):\n",
    "    \"\"\"Generate translations for given source texts\"\"\"\n",
    "    model.eval()\n",
    "    all_translations = []\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, len(source_texts), batch_size):\n",
    "        batch_texts = source_texts[i:i+batch_size]\n",
    "\n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Move to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {key: val.cuda() for key, val in inputs.items()}\n",
    "\n",
    "        # Generate translations\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3,\n",
    "                do_sample=False,\n",
    "                temperature=1.0\n",
    "            )\n",
    "\n",
    "        # Decode translations\n",
    "        batch_translations = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        all_translations.extend(batch_translations)\n",
    "\n",
    "        print(f\"Processed {i+len(batch_texts)}/{len(source_texts)} samples\")\n",
    "\n",
    "    return all_translations\n",
    "\n",
    "def create_translation_submission(ids, translations, filename=\"answer.csv\"):\n",
    "    \"\"\"Create submission file in required CSV format\"\"\"\n",
    "\n",
    "    submission_df = pd.DataFrame({\n",
    "        'ID': ids,\n",
    "        'Translation': translations\n",
    "    })\n",
    "\n",
    "    submission_df.to_csv(filename, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"Submission file created: {filename}\")\n",
    "    print(f\"Total entries: {len(ids)}\")\n",
    "    print(f\"Sample entries:\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "print(\" Inference functions defined!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =ag==================================================================\n",
    "# CELL 10: Check GPU and Prepare Data\n",
    "# ===================================================================\n",
    "# The Colab-specific instruction for enabling a GPU has been updated for Kaggle.\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    # These two lines must be indented to be inside the 'if' block\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    # Updated message for the Kaggle environment\n",
    "    print(\"✗ No GPU detected! Please enable the T4 GPU in the notebook settings (Accelerator option).\")\n",
    "\n",
    "# Prepare data\n",
    "print(\"\\nPreparing translation data...\")\n",
    "train_source, train_target, val_source, val_ids = prepare_translation_data(train_df, val_df)\n",
    "\n",
    "if train_source is None:\n",
    "    print(\"✗ Error in data preparation. Please check your column names.\")\n",
    "else:\n",
    "    print(f\"✓ Data prepared successfully!\")\n",
    "    print(f\"  Training pairs: {len(train_source)}\")\n",
    "    print(f\"  Validation samples: {len(val_source)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CELL 11: Setup Model and Create Dataset\n",
    "# ===================================================================\n",
    "# This is the missing cell that creates the 'train_dataset' variable.\n",
    "\n",
    "# Setup model and tokenizer\n",
    "print(\"Setting up translation model...\")\n",
    "model, tokenizer, model_name = setup_translation_model()\n",
    "\n",
    "# Move model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print(\"✓ Model moved to GPU\")\n",
    "\n",
    "# Create the tokenized training dataset\n",
    "print(\"\\nCreating training dataset...\")\n",
    "train_dataset = create_translation_dataset(train_source, train_target, tokenizer)\n",
    "\n",
    "print(\"\\n✓ Model and dataset ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CELL 12: Start Training (This will take 2-3 hours) - KAGGLE VERSION\n",
    "# ===================================================================\n",
    "# The Google Drive mount has been removed.\n",
    "# All outputs are now saved to the standard Kaggle output directory.\n",
    "\n",
    "\n",
    "# Define the output path in the Kaggle working directory\n",
    "output_dir = \"/kaggle/working/lmt-final-model\"\n",
    "\n",
    "# Updated message for Kaggle's background execution\n",
    "print(\"Starting training... \")\n",
    "print(\"You can 'Save Version' to run this in the background. You don't need to keep the tab open!\")\n",
    "\n",
    "# The core training function logic is already compatible.\n",
    "def train_translation_model_fixed(train_dataset, model, tokenizer, model_name, output_dir):\n",
    "    \"\"\"Train the translation model with corrected arguments\"\"\"\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Training arguments pointing to the Kaggle output directory\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=24,\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=6250,\n",
    "        weight_decay=0.02,\n",
    "        warmup_ratio=0.1,    # 10% warmup\n",
    "        lr_scheduler_type=\"cosine_with_restarts\",  # Better than linear\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=500,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=500,\n",
    "        save_total_limit=8,\n",
    "        fp16=True,\n",
    "        learning_rate=3e-5,\n",
    "        remove_unused_columns=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        load_best_model_at_end=False,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    print(\"\\nStarting training process...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the final model\n",
    "    print(\"\\nSaving final model...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    print(f\"\\n✓ Training completed! Model saved to {output_dir}\")\n",
    "    return trainer\n",
    "\n",
    "# Train the model, passing the correct Kaggle output path\n",
    "trainer = train_translation_model_fixed(train_dataset, model, tokenizer, model_name, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T03:32:29.754174Z",
     "iopub.status.busy": "2025-10-08T03:32:29.753603Z",
     "iopub.status.idle": "2025-10-08T03:32:29.825368Z",
     "shell.execute_reply": "2025-10-08T03:32:29.824494Z",
     "shell.execute_reply.started": "2025-10-08T03:32:29.754140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CELL 13: Run Model on Full Validation Dataset - KAGGLE VERSION\n",
    "# ===================================================================\n",
    "\n",
    "print(\"Running trained model on full validation dataset...\")\n",
    "print(f\"Validation samples: {len(val_source)}\")\n",
    "\n",
    "# Load the trained model from the Kaggle working directory.\n",
    "# This MUST exactly match the 'output_dir' from your training cell.\n",
    "model_path = \"/kaggle/working/lmt-final-model\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print(\"✓ Model loaded on GPU\")\n",
    "\n",
    "# Generate translations for the entire validation set\n",
    "print(\"\\nGenerating translations... This may take 10-15 minutes\")\n",
    "val_translations = translate_texts(model, tokenizer, val_source, val_ids, batch_size=16)\n",
    "\n",
    "print(\"\\n✓ Validation translations completed!\")\n",
    "print(f\"  Generated {len(val_translations)} translations\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nSample validation results:\")\n",
    "for i in range(min(10, len(val_translations))):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"  ID: {val_ids[i]}\")\n",
    "    print(f\"  Source: {val_source[i][:100]}{'...' if len(val_source[i]) > 100 else ''}\")\n",
    "    print(f\"  Translation: {val_translations[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-07T21:15:02.830289Z",
     "iopub.status.idle": "2025-10-07T21:15:02.830616Z",
     "shell.execute_reply": "2025-10-07T21:15:02.830464Z",
     "shell.execute_reply.started": "2025-10-07T21:15:02.830449Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CELL 14: Create Validation Submission File\n",
    "# ===================================================================\n",
    "# This cell is already compatible with Kaggle's file system.\n",
    "\n",
    "print(\"Creating validation submission file...\")\n",
    "\n",
    "# This function saves files to /kaggle/working/\n",
    "create_translation_submission(val_ids, val_translations, \"validation_predictions.csv\")\n",
    "\n",
    "# This will also be saved to /kaggle/working/\n",
    "create_translation_submission(val_ids, val_translations, \"answer.csv\")\n",
    "\n",
    "# --- Verification Step ---\n",
    "import pandas as pd\n",
    "submission_df = pd.read_csv(\"/kaggle/working/answer.csv\")\n",
    "\n",
    "print(f\"\\nValidation Submission Summary:\")\n",
    "print(f\"Total predictions: {len(submission_df)}\")\n",
    "print(f\"Unique IDs: {submission_df['ID'].nunique()}\")\n",
    "print(f\"Average translation length: {submission_df['Translation'].str.len().mean():.1f} characters\")\n",
    "\n",
    "print(\"\\nFirst 5 predictions:\")\n",
    "print(submission_df.head()) "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8425470,
     "sourceId": 13293438,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
